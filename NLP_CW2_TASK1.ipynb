{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFuT4wLXAv8e14x2a/ahWY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukasRimkus/100-Prisoners-Riddle/blob/main/NLP_CW2_TASK1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing Coursework 2 - Task 1"
      ],
      "metadata": {
        "id": "Oj4ByZUQ7NMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open Google Colab"
      ],
      "metadata": {
        "id": "rXga-NaD7Zcx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxqSLk4N69yA",
        "outputId": "ebe9adcc-df35-4cff-f7aa-122f7f825134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "colab_path = '/content/drive'\n",
        "drive.mount(colab_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Essential Libraries\n",
        "\n",
        "Import libraries used in the coursework"
      ],
      "metadata": {
        "id": "Lx0B4zfn77l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.cluster import euclidean_distance, cosine_distance\n",
        "\n",
        "from collections import Counter\n",
        "import time\n",
        "import itertools\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, Birch\n",
        "from sklearn.model_selection import train_test_split "
      ],
      "metadata": {
        "id": "HJoOer3C77VZ"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download required sets and set the constants. "
      ],
      "metadata": {
        "id": "R99o0xcs9Q1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/product_reviews\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFvOc_9-9Ql9",
        "outputId": "96648a6c-c427-4e7d-af1a-c0fa3a6fd936"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Contraction Map Definition\n",
        "\n",
        "The defined dictionary is used for expanding contractions (like **can't** is replaced by **cannot**) inside the documents."
      ],
      "metadata": {
        "id": "2eaZo5Q29olc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Contractions Map \n",
        "\"\"\"\n",
        "This snippet of code was taken from:\n",
        "https://github.com/kootenpv/contractions/blob/master/contractions/data/contractions_dict.json\n",
        "\n",
        "As the library could not be downloaded to do it, so I found handy to make use of it\n",
        "to remove unnecessary bits of code quite easily with minimal invested resources. \n",
        "\"\"\"\n",
        "\n",
        "CONTRACTIONS = {\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I'm'a\": \"I am about to\",\n",
        "    \"I'm'o\": \"I am going to\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'd've\": \"I would have\",\n",
        "    \"Whatcha\": \"What are you\",\n",
        "    \"amn't\": \"am not\",\n",
        "    \"ain't\": \"are not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"daren't\": \"dare not\",\n",
        "    \"daresn't\": \"dare not\",\n",
        "    \"dasn't\": \"dare not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"didn’t\": \"did not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"don’t\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"e'er\": \"ever\",\n",
        "    \"everyone's\": \"everyone is\",\n",
        "    \"finna\": \"fixing to\",\n",
        "    \"gimme\": \"give me\",\n",
        "    \"gon't\": \"go not\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he've\": \"he have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"how're\": \"how are\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"'tis\": \"it is\",\n",
        "    \"'twas\": \"it was\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"kinda\": \"kind of\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"luv\": \"love\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"may've\": \"may have\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"ne'er\": \"never\",\n",
        "    \"o'\": \"of\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"ol'\": \"old\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"o'er\": \"over\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shalln't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"somebody's\": \"somebody is\",\n",
        "    \"someone's\": \"someone is\",\n",
        "    \"something's\": \"something is\",\n",
        "    \"sux\": \"sucks\",\n",
        "    \"that're\": \"that are\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"that'll\": \"that will\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"em\": \"them\",\n",
        "    \"there're\": \"there are\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"there'll\": \"there will\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"these're\": \"these are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"this's\": \"this is\",\n",
        "    \"this'll\": \"this will\",\n",
        "    \"this'd\": \"this would\",\n",
        "    \"those're\": \"those are\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what'd\": \"what did\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"where're\": \"where are\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"which's\": \"which is\",\n",
        "    \"who're\": \"who are\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who'd\": \"who would\",\n",
        "    \"who'd've\": \"who would have\",\n",
        "    \"why're\": \"why are\",\n",
        "    \"why'd\": \"why did\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"you'll've\": \"you shall have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rgsfz9R49rUr"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "cRF_nuyzeM1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_OF_WORDS = 50"
      ],
      "metadata": {
        "id": "A9EfZftYeLgJ"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing Class Definition \n",
        "\n",
        "Define Pre-processing class for cleaning the document text, converting it to cleaned tokens and adding multiwords to the index. "
      ],
      "metadata": {
        "id": "b8fjjWmP_jV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessing:\n",
        "    \"\"\"\n",
        "    This is the class used to pre-process documents and search terms. \n",
        "    I found this more readable and comfortable to move this functionality to a \n",
        "    separate class than to have everything in one place. This is a more typical \n",
        "    approach used in industry.\n",
        "    \"\"\"\n",
        "    \n",
        "    def lower_doc(self, doc: str) -> str:\n",
        "        \"\"\"\n",
        "        This method takes a document as string and converts all characters to lowercase.\n",
        "        Return a lowercased doocument.\n",
        "        \"\"\"\n",
        "        doc = doc.lower()\n",
        "        return doc\n",
        "\n",
        "    def remove_non_ascii_characters(self, doc: str) -> str:\n",
        "        \"\"\"\n",
        "        This method removes non-ASCII characters from the document text.\n",
        "        It has a document as string parameter.\n",
        "        Returns a document with removed non-ASCII characters which are replaced by a space.\n",
        "        \"\"\"\n",
        "        # Without an added space, the words could be merged together to one, which is unwanted.\n",
        "        doc = re.sub(r'[^\\x00-\\x7F]+',' ', doc)\n",
        "        return doc\n",
        "    \n",
        "    def remove_annotations(self, document: str) -> str:\n",
        "        \"\"\"\n",
        "        This method removes citations like \"[1]\" from the document text.\n",
        "\n",
        "        It has a document as string parameter.\n",
        "        Returns a document with removed citations.\n",
        "        \"\"\"\n",
        "        document = re.sub(r\"\\[.{0,5}\\]\", \" \", document)\n",
        "\n",
        "        return document\n",
        "\n",
        "    def remove_unnecessary_symbols(self, document: str) -> str:\n",
        "        document = document.replace(\"##\", \" \")\n",
        "        document = document.replace(\"\\n\", \" \")\n",
        "        return document\n",
        "\n",
        "    def remove_punctuation(self, sents: list) -> list:\n",
        "        \"\"\"\n",
        "        This method removes punctuation from the document text.\n",
        "\n",
        "        It has a document as string parameter.\n",
        "        Returns a document with removed punctuations.\n",
        "        \"\"\"\n",
        "\n",
        "        # Remove an apostrophe from the list as they are removed later after removing contractions.\n",
        "        punctuation = string.punctuation.replace(\"'\", \"\")\n",
        "        # punctuation = string.punctuation\n",
        "\n",
        "        # It is actually faster to use a for loop for this than regex - \n",
        "        # https://datagy.io/python-remove-punctuation-from-string/. \n",
        "        # I do not use maketrans as I need to replace a punctuation by a space.\n",
        "        for i, sent in enumerate(sents):\n",
        "            for punct in punctuation:\n",
        "                sent = sent.replace(punct, \" \")\n",
        "            \n",
        "            sents[i] = sent\n",
        "\n",
        "        return sents\n",
        "\n",
        "    def remove_digits(self, sentences: list) -> list:\n",
        "        \"\"\"\n",
        "        This method removes digits from the document text.\n",
        "\n",
        "        It has a document as string parameter.\n",
        "        Returns a document with removed digits.\n",
        "        \"\"\"\n",
        "        for i, sent in enumerate(sentences):\n",
        "            sentences[i] = re.sub(\"\\d+\", \"\", sent)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def replace_contractions(self, sentences: list) -> list:\n",
        "        \"\"\"\n",
        "        This method replaces contractions in the document text using a contraction map\n",
        "        which is taken from a library on github repository. I found this a better \n",
        "        way to deal with contractions. The cost of this is small as there a lot of \n",
        "        contractions and the whole dictionary is quite small.\n",
        "\n",
        "        It has a document as string parameter.\n",
        "        Returns a document with replaced contraction equivalents.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Split the document to tokens separated by a space.  \n",
        "        for i, sent in enumerate(sentences):\n",
        "\n",
        "            words = sent.split()\n",
        "            filtered_words = list()\n",
        "            \n",
        "            for i, word in enumerate(words):\n",
        "                if word in CONTRACTIONS:\n",
        "                    filtered_words.extend(CONTRACTIONS[word].split())\n",
        "                else:\n",
        "                    filtered_words.append(word)\n",
        "            \n",
        "            # rejoin those tokens to a string\n",
        "            sentences[i] = \" \".join(filtered_words)\n",
        "        \n",
        "        return sentences\n",
        "\n",
        "    def remove_apostrophes(self, sentences: list) -> list:\n",
        "        \"\"\"\n",
        "        This method removes apostrophes from the document text. This is only done \n",
        "        after contractions expansion as ti should handle all the left cases with \n",
        "        unremoved apostrophes.\n",
        "\n",
        "        It has a document as string parameter.\n",
        "        Returns a document with removed apostrophes.\n",
        "        \"\"\"\n",
        "        for i, sent in enumerate(sentences):\n",
        "            sentences[i] = sent.replace(\"'\", \" \")\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def do_tokenisation(self, sents: list) -> list:\n",
        "        tokens = [word_tokenize(sent) for sent in sents]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def do_stemming(self, tokens_lists: list) -> list:\n",
        "        \"\"\"\n",
        "        This method does stemming for each token.\n",
        "\n",
        "        It has tokens as a list parameter.\n",
        "        Returns a list of stemmed tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [[stemmer.stem(token) for token in tokens] for tokens in tokens_lists]\n",
        "        return tokens\n",
        "\n",
        "    def do_lemmatisation(self, tokens_lists: list) -> list:\n",
        "        lemmatiser = WordNetLemmatizer()\n",
        "        tokens = [[lemmatiser.lemmatize(token) for token in tokens] for tokens in tokens_lists]\n",
        "        return tokens\n",
        "\n",
        "    def remove_stopwords(self, tokens_lists: list) -> list:\n",
        "        \"\"\"\n",
        "        This method removes words which do not give much value to index which are \n",
        "        known as stopwords. Also, words of size less than 2 characters are removed as well. \n",
        "        This method is used for cleaned document tokens with positions or only tokens for \n",
        "        cleaned search queries.\n",
        "\n",
        "        It has tokens as a list parameter and a boolean value for determining if \n",
        "        the list contains only tokens or tuples of tokens and positions. \n",
        "        Returns a list of tokens with removed stopwords and tokens of size less than \n",
        "        2 characters.\n",
        "        \"\"\"\n",
        "\n",
        "        # I may add my own stopwords\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "        extra_stopwords = [\"was\", \"would\", \"could\", \"need\", \"has\"]\n",
        "        stopwords += extra_stopwords\n",
        "\n",
        "        # Word \"won\" has a semantic meaning in the documents, especially when I expand \n",
        "        # \"won't\" to \"will not\", as the motivation to add \"won\" to stopwords list might have \n",
        "        # been to remove a stemmed form of \"won't\". But in my case, it removes a semantically\n",
        "        # important word.\n",
        "        tokens = [[token for token in tokens if token not in stopwords and len(token) > 1] for tokens in tokens_lists]\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "    def process_text(self, document: str) -> str:\n",
        "        \"\"\"\n",
        "        This method makes use of methods created above for cleaning a text document.\n",
        "        It does these cleaning steps:\n",
        "        1) Making the text to the lower case.\n",
        "        2) Removing redundant phrases.\n",
        "        3) Removing non-ASCII characters.\n",
        "        4) Replacing citations.\n",
        "        5) Removing punctuation.\n",
        "        6) Removing digits.\n",
        "        7) Expanding contractions.\n",
        "        8) Removing apostrophes.\n",
        "\n",
        "        It has a unprocessed document string as its parameter. \n",
        "        Returns a cleaned document ready for tokenisation and stemming.\n",
        "        \"\"\"\n",
        "\n",
        "        # make the case to lowercase for each word\n",
        "        # may not need this \n",
        "        document = self.lower_doc(document)\n",
        "\n",
        "        # remove non ascii characters from the document\n",
        "        document = self.remove_non_ascii_characters(document)\n",
        "\n",
        "        # remove annotations like '[+1]'\n",
        "        document = self.remove_annotations(document)\n",
        "\n",
        "        # remove some unnecessary symbols\n",
        "        document = self.remove_unnecessary_symbols(document)\n",
        "\n",
        "        # do sentence tokenisation here!\n",
        "        sentences = sent_tokenize(document)\n",
        "\n",
        "        # remove punctuation (remove apostrophes later after removing stopwords)\n",
        "        sentences = self.remove_punctuation(sentences)\n",
        "\n",
        "        # remove digits\n",
        "        sentences = self.remove_digits(sentences)\n",
        "\n",
        "        # replace contractions using a map which may not be detected by the tokenizer\n",
        "        sentences = self.replace_contractions(sentences)\n",
        "\n",
        "        # remove existing apostrophes after removing contractions\n",
        "        sentences = self.remove_apostrophes(sentences)\n",
        "        \n",
        "        return sentences\n",
        "\n",
        "    def process_document(self, document: str) -> list:\n",
        "        \"\"\"\n",
        "        This method does the whole pre-processing for the document which becomes \n",
        "        ready for the building of the inverted index. \n",
        "        It does these steps:\n",
        "        1) Cleaning the text like removing unnecessary text (look at 'process_text' method).\n",
        "        2) Tokenisation of the cleaned string of text.\n",
        "        3) Adding positions for tokens.\n",
        "        4) Stemming tokens.\n",
        "        5) Removing stopwords.\n",
        "        6) Adding multiword terms from csv files.\n",
        "\n",
        "        It has a string of a document and a list of csv multiword terms as its parameters. \n",
        "        Returns a list of tuples of terms and positions.\n",
        "        \"\"\"\n",
        "        # prepare the document for the tokenisation\n",
        "        sentences = self.process_text(document)\n",
        "\n",
        "        # do tokenization of the cleaned text\n",
        "        tokens_lists = self.do_tokenisation(sentences)\n",
        "\n",
        "        # remove stopwords and single char tokens\n",
        "        tokens_lists = self.remove_stopwords(tokens_lists)\n",
        "\n",
        "        # do stemming of the tokens\n",
        "        # tokens_lists = self.do_stemming(tokens_lists)\n",
        "        tokens_lists = self.do_lemmatisation(tokens_lists)\n",
        "\n",
        "        return tokens_lists\n",
        "\n",
        "\n",
        "preprocessing = PreProcessing()"
      ],
      "metadata": {
        "id": "ka87bzsb7YrG"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read Data"
      ],
      "metadata": {
        "id": "1i52W8thD7TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path: str) -> list:\n",
        "    \"\"\"\n",
        "    This method read files from a directory and then adds the data of each file\n",
        "    into a list.\n",
        "\n",
        "    It has a string of a path to read data from as its parameter. \n",
        "    Returns a list of tuples of a document text string and a tuple of the season \n",
        "    and episode integers.\n",
        "    \"\"\"\n",
        "\n",
        "    def read_single_file(file):\n",
        "        \"\"\"\n",
        "        This method read text for a given file.\n",
        "\n",
        "        It has a string of a file name to read data from as its parameter. \n",
        "        Returns a title of the episode and the whole document text for pre-processing.\n",
        "        Returns None if an exception was caught.\n",
        "        \"\"\"\n",
        "        file_path = f\"{path}/{file}\"\n",
        "        try:\n",
        "            with open(file_path, 'r') as fd:\n",
        "                text = fd.read()\n",
        "                return text\n",
        "\n",
        "        except IOError as e: \n",
        "            print(f\"There was an IOError failure reading a text file at {file_path}. Error message: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"There was some unrecognised failure reading a text file at {file_path}. Error message: {e}\")\n",
        "        \n",
        "\n",
        "    # Access the directory given in path.\n",
        "    all_file_names = os.listdir(path)\n",
        "    text_file_names = list(filter(lambda x: x[-4:] == '.txt' and x != \"README.txt\", all_file_names))\n",
        "\n",
        "    # Read the files and extract text and title for each document.\n",
        "    documents = list(map(read_single_file, text_file_names)) \n",
        "\n",
        "    # remove .txt extension as it gives no information after finding those documents\n",
        "    # text_file_names = [name[:-4].split(\".\") for name in text_file_names]\n",
        "    \n",
        "    return documents\n",
        "\n",
        "\n",
        "def get_corpus_in_sentences():\n",
        "    reviews = read_data(BASE_PATH)\n",
        "    corpus = \"\\n\".join(reviews)\n",
        "    tokenised_sentence_corpus = preprocessing.process_document(corpus)\n",
        "\n",
        "    tokenised_corpus = list()\n",
        "    for sent in tokenised_sentence_corpus:\n",
        "        tokenised_corpus.extend(sent)\n",
        "\n",
        "    return tokenised_sentence_corpus, tokenised_corpus\n"
      ],
      "metadata": {
        "id": "V0e0YnyA_m1T"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_common_words(tokenised_corpus):\n",
        "    all_word_dist = nltk.FreqDist(tokenised_corpus)\n",
        "    most_common_words = [word for (word, _) in all_word_dist.most_common(NUMBER_OF_WORDS)]\n",
        "    print(f\"Most common words:\\n{most_common_words}\")\n",
        "\n",
        "    most_common_words = np.array(most_common_words)\n",
        "    tokenised_corpus = np.array(tokenised_corpus)\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "def get_reversed_most_common_words(most_common_words):\n",
        "    reversed_words = [word[::-1] for word in most_common_words]\n",
        "    print(f\"\\nMost common reversed words:\\n{reversed_words}\")\n",
        "    return reversed_words\n",
        "\n",
        "def get_all_words(most_common_words, reversed_words):\n",
        "    all_words = list()\n",
        "    for word, reversed_word in zip(most_common_words, reversed_words):\n",
        "        all_words.append(word)\n",
        "        all_words.append(reversed_word)\n",
        "    return all_words"
      ],
      "metadata": {
        "id": "bL5LNQLuGCXJ"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenised_sentence_corpus, tokenised_corpus = get_corpus_in_sentences()\n",
        "most_common_words = get_most_common_words(tokenised_corpus)\n",
        "reversed_words = get_reversed_most_common_words(most_common_words)\n",
        "all_words = get_all_words(most_common_words, reversed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE6tSIdlgj-3",
        "outputId": "53135660-89d6-4a73-8ac3-9c047e4c2dc5"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words:\n",
            "['phone', 'use', 'one', 'ipod', 'router', 'player', 'camera', 'get', 'battery', 'diaper', 'work', 'product', 'great', 'like', 'feature', 'time', 'good', 'zen', 'problem', 'quality', 'also', 'sound', 'computer', 'software', 'really', 'micro', 'well', 'take', 'picture', 'thing', 'easy', 'even', 'used', 'creative', 'first', 'much', 'want', 'better', 'bag', 'mp', 'champ', 'look', 'size', 'go', 'music', 'norton', 'support', 'buy', 'price', 'little']\n",
            "\n",
            "Most common reversed words:\n",
            "['enohp', 'esu', 'eno', 'dopi', 'retuor', 'reyalp', 'aremac', 'teg', 'yrettab', 'repaid', 'krow', 'tcudorp', 'taerg', 'ekil', 'erutaef', 'emit', 'doog', 'nez', 'melborp', 'ytilauq', 'osla', 'dnuos', 'retupmoc', 'erawtfos', 'yllaer', 'orcim', 'llew', 'ekat', 'erutcip', 'gniht', 'ysae', 'neve', 'desu', 'evitaerc', 'tsrif', 'hcum', 'tnaw', 'retteb', 'gab', 'pm', 'pmahc', 'kool', 'ezis', 'og', 'cisum', 'notron', 'troppus', 'yub', 'ecirp', 'elttil']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best way to do hyperparameter selection?"
      ],
      "metadata": {
        "id": "3TvK3AaVVV9F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dja2P40uhnNf"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment"
      ],
      "metadata": {
        "id": "8VDbU9qlhoR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributionalSemanticsExperiment:\n",
        "    NUMBER_OF_WORDS = 50\n",
        "\n",
        "    def __init__(self, random_trials, tokenised_sentence_corpus, all_words, most_common_words, reversed_words, N, d, window, min_count, iter, alpha):\n",
        "        self.random_trials = random_trials\n",
        "        self.tokenised_sentence_corpus = tokenised_sentence_corpus\n",
        "        self.all_words = all_words\n",
        "        self.most_common_words = most_common_words\n",
        "        self.reversed_words = reversed_words\n",
        "        self.N = N\n",
        "        self.d = d\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.iter = iter\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def get_results(self):\n",
        "        percentages = self.do_experiment_given_number_of_times()\n",
        "\n",
        "        mean, std = percentages.mean(), percentages.std()\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "    def do_experiment_given_number_of_times(self):\n",
        "        percentages = np.zeros(self.random_trials)\n",
        "        tokenised_sentence_corpus = copy.deepcopy(self.tokenised_sentence_corpus)\n",
        "\n",
        "        for i in range(self.random_trials):\n",
        "            tokenised_sentence_corpus = self.do_random_word_subsampling(tokenised_sentence_corpus)\n",
        "\n",
        "            words_embeddings = self.generate_word_embeddings(tokenised_sentence_corpus)\n",
        "\n",
        "            result = self.get_clustering_results(words_embeddings)\n",
        "\n",
        "            percentage = self.get_accuracy(result)\n",
        "            percentages[i] = percentage\n",
        "\n",
        "            tokenised_sentence_corpus = copy.deepcopy(self.tokenised_sentence_corpus)\n",
        "\n",
        "        return percentages\n",
        "\n",
        "\n",
        "    def do_random_word_subsampling(self, tokenised_sentence_corpus):\n",
        "        for word, reversed_word in zip(self.most_common_words, self.reversed_words):\n",
        "            indices = [(i, j) for i, sent in enumerate(tokenised_sentence_corpus) for j, token in enumerate(sent) if token == word]\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            # get half of word occurrences which will be replaced by the pseudo word\n",
        "            number_of_indices = len(indices)//2\n",
        "\n",
        "            # second half will be replaced by the reversed word\n",
        "            replace_by_pseudo_words = indices[number_of_indices:]\n",
        "\n",
        "            for (x, y) in replace_by_pseudo_words:\n",
        "                tokenised_sentence_corpus[x][y] = reversed_word\n",
        "\n",
        "        return tokenised_sentence_corpus\n",
        "\n",
        "\n",
        "    def generate_word_embeddings(self, tokenised_sentence_corpus):\n",
        "        model = Word2Vec(sentences=tokenised_sentence_corpus, size=self.d, window=self.window, min_count=self.min_count, iter=self.iter, alpha=self.alpha, ns_exponent=0.75)\n",
        "        # model = FastText(sentences=tokenised_sentence_corpus, size=d, window=5, min_count=5, workers=20, iter=100, ns_exponent=0.75)\n",
        "\n",
        "        words_embeddings = np.zeros([self.N, self.d])\n",
        "\n",
        "        for i, word in enumerate(self.all_words):\n",
        "            words_embeddings[i] = model.wv[word]\n",
        "        \n",
        "        return words_embeddings\n",
        "\n",
        "\n",
        "    def get_clustering_results(self, words_embeddings):\n",
        "        # clusterer = nltk.cluster.GAAClusterer(N//2)\n",
        "        # result = clusterer.cluster(words_embeddings, assign_clusters=True)\n",
        "\n",
        "        # clusterer = KMeans(n_clusters=N//2)\n",
        "        clusterer = AgglomerativeClustering(n_clusters=NUMBER_OF_WORDS, linkage=\"ward\", affinity=\"euclidean\")\n",
        "        # clusterer = AgglomerativeClustering(n_clusters=N//2, linkage=\"complete\", affinity=\"cosine\")\n",
        "        # clusterer = AgglomerativeClustering(n_clusters=N//2, linkage=\"average\", affinity=\"cosine\")\n",
        "        # clusterer = Birch(n_clusters=N//2)\n",
        "        \n",
        "        result = clusterer.fit_predict(words_embeddings)\n",
        "\n",
        "        # check if there are none empty clusters!\n",
        "        # calculate rand score? \n",
        "        # https://scikit-learn.org/stable/modules/clustering.html#birch\n",
        "        # print(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def get_accuracy(self, result):\n",
        "        matches = 0\n",
        "        for i in range(0, self.N, 2):\n",
        "            if result[i] == result[i+1]:\n",
        "                matches += 1\n",
        "\n",
        "        percentage = matches/NUMBER_OF_WORDS\n",
        "        return percentage\n"
      ],
      "metadata": {
        "id": "wswsUFPtHHsw"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_trials = 10\n",
        "d = 200\n",
        "N = NUMBER_OF_WORDS*2\n",
        "window = 4\n",
        "min_count = 2\n",
        "iter = 100\n",
        "alpha = 0.02\n",
        "\n",
        "start = time.time()\n",
        "experiment = DistributionalSemanticsExperiment(random_trials=random_trials, tokenised_sentence_corpus=copy.deepcopy(tokenised_sentence_corpus), \\\n",
        "                                               all_words=all_words, most_common_words=most_common_words, reversed_words=reversed_words, \\\n",
        "                                               N=N, d=d, window=window, min_count=min_count, iter=iter, alpha=alpha)\n",
        "\n",
        "mean, std = experiment.get_results()\n",
        "print(time.time() - start)\n",
        "mean, std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tcpgrtI3O5R",
        "outputId": "0b929fd1-fbd7-4986-a622-b4bd78c411a0"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74.52502179145813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9279999999999999, 0.018330302779823338)"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.wv.most_similar(\"good\")"
      ],
      "metadata": {
        "id": "c57RTHLdvea_"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model)\n",
        "# print(model.wv.vocab)"
      ],
      "metadata": {
        "id": "wjuhXaId1xvE"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Selection\n",
        "\n",
        "# For results create a dictionary of items where the key is the tuple of all parameters and the value is the percentage received\n",
        "results = dict()\n",
        "random_trials = 4\n",
        "N = NUMBER_OF_WORDS*2\n",
        "\n",
        "\n",
        "def hyperparameter_selection(d_values, window_sizes, min_counts, iter_numbers, alpha_values):\n",
        "    for d in d_values:\n",
        "        for window in window_sizes:\n",
        "            for min_count in min_counts:\n",
        "                for iter in iter_numbers:\n",
        "                    for alpha in alpha_values:\n",
        "                        # parameters = (d, window, min_count, iter, alpha)\n",
        "                        parameters = f\"d={d}, window={window}, min_count={min_count}, iter={iter}, alpha={alpha}\"\n",
        "\n",
        "                        experiment = DistributionalSemanticsExperiment(random_trials=random_trials, tokenised_sentence_corpus=copy.deepcopy(tokenised_sentence_corpus), \\\n",
        "                                                                    all_words=all_words, most_common_words=most_common_words, reversed_words=reversed_words, \\\n",
        "                                                                    N=N, d=d, window=window, min_count=min_count, iter=iter, alpha=alpha)\n",
        "\n",
        "                        result = experiment.get_results()\n",
        "\n",
        "                        results[parameters] = result\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# 17280 unique combinations - would take about 216 hours...\n",
        "# d_values = [50, 100, 150, 200, 250, 300]\n",
        "# 100 was not working well\n",
        "d_values = [100, 200, 300]\n",
        "\n",
        "# window_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "window_sizes = [3, 4, 5, 6]\n",
        "\n",
        "# min_counts = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "min_counts = [2, 3, 4]\n",
        "\n",
        "# 200 was too much for my model\n",
        "# iter_numbers = [50, 100, 150, 200, 250, 300]\n",
        "iter_numbers = [100, 150]\n",
        "\n",
        "# alpha_values = [0.001, 0.005, 0.01, 0.025, 0.05, 0.1]\n",
        "alpha_values = [0.02, 0.025]\n",
        "\n",
        "# ns_exponent_values = [-1, -0.5, 0, 0.5, 0.75, 1]\n",
        "# hs_values = [0, 1]\n",
        "# sg_values = [0, 1] \n",
        "\n",
        "start = time.time()\n",
        "results = hyperparameter_selection(d_values, window_sizes, min_counts, iter_numbers, alpha_values)\n",
        "print(time.time() - start)\n",
        "\n",
        "results, len(results)"
      ],
      "metadata": {
        "id": "-An3szGPjqwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc27454-d0de-4e70-fb91-d9191708dd61"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4876.3112506866455\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'d=100, window=3, min_count=2, iter=100, alpha=0.02': (0.9249999999999999,\n",
              "   0.02958039891549806),\n",
              "  'd=100, window=3, min_count=2, iter=100, alpha=0.025': (0.91,\n",
              "   0.01732050807568879),\n",
              "  'd=100, window=3, min_count=2, iter=150, alpha=0.02': (0.905,\n",
              "   0.016583123951777013),\n",
              "  'd=100, window=3, min_count=2, iter=150, alpha=0.025': (0.8300000000000001,\n",
              "   0.03),\n",
              "  'd=100, window=3, min_count=3, iter=100, alpha=0.02': (0.915,\n",
              "   0.01658312395177697),\n",
              "  'd=100, window=3, min_count=3, iter=100, alpha=0.025': (0.88,\n",
              "   0.014142135623730963),\n",
              "  'd=100, window=3, min_count=3, iter=150, alpha=0.02': (0.8899999999999999,\n",
              "   0.02999999999999998),\n",
              "  'd=100, window=3, min_count=3, iter=150, alpha=0.025': (0.86,\n",
              "   0.020000000000000018),\n",
              "  'd=100, window=3, min_count=4, iter=100, alpha=0.02': (0.8999999999999999,\n",
              "   0.028284271247461888),\n",
              "  'd=100, window=3, min_count=4, iter=100, alpha=0.025': (0.895,\n",
              "   0.016583123951777013),\n",
              "  'd=100, window=3, min_count=4, iter=150, alpha=0.02': (0.855,\n",
              "   0.029580398915498105),\n",
              "  'd=100, window=3, min_count=4, iter=150, alpha=0.025': (0.865,\n",
              "   0.021794494717703387),\n",
              "  'd=100, window=4, min_count=2, iter=100, alpha=0.02': (0.9149999999999999,\n",
              "   0.01658312395177697),\n",
              "  'd=100, window=4, min_count=2, iter=100, alpha=0.025': (0.905,\n",
              "   0.016583123951777013),\n",
              "  'd=100, window=4, min_count=2, iter=150, alpha=0.02': (0.905,\n",
              "   0.05172040216394301),\n",
              "  'd=100, window=4, min_count=2, iter=150, alpha=0.025': (0.88,\n",
              "   0.03741657386773945),\n",
              "  'd=100, window=4, min_count=3, iter=100, alpha=0.02': (0.91,\n",
              "   0.01732050807568879),\n",
              "  'd=100, window=4, min_count=3, iter=100, alpha=0.025': (0.905,\n",
              "   0.025980762113533142),\n",
              "  'd=100, window=4, min_count=3, iter=150, alpha=0.02': (0.885,\n",
              "   0.008660254037844395),\n",
              "  'd=100, window=4, min_count=3, iter=150, alpha=0.025': (0.875,\n",
              "   0.029580398915498105),\n",
              "  'd=100, window=4, min_count=4, iter=100, alpha=0.02': (0.9049999999999999,\n",
              "   0.049749371855331),\n",
              "  'd=100, window=4, min_count=4, iter=100, alpha=0.025': (0.9,\n",
              "   0.042426406871192854),\n",
              "  'd=100, window=4, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "   0.029580398915498105),\n",
              "  'd=100, window=4, min_count=4, iter=150, alpha=0.025': (0.825,\n",
              "   0.029580398915498064),\n",
              "  'd=100, window=5, min_count=2, iter=100, alpha=0.02': (0.895,\n",
              "   0.02598076211353318),\n",
              "  'd=100, window=5, min_count=2, iter=100, alpha=0.025': (0.8999999999999999,\n",
              "   0.042426406871192854),\n",
              "  'd=100, window=5, min_count=2, iter=150, alpha=0.02': (0.86,\n",
              "   0.042426406871192854),\n",
              "  'd=100, window=5, min_count=2, iter=150, alpha=0.025': (0.88,\n",
              "   0.020000000000000018),\n",
              "  'd=100, window=5, min_count=3, iter=100, alpha=0.02': (0.9,\n",
              "   0.024494897427831803),\n",
              "  'd=100, window=5, min_count=3, iter=100, alpha=0.025': (0.895,\n",
              "   0.03570714214271425),\n",
              "  'd=100, window=5, min_count=3, iter=150, alpha=0.02': (0.89,\n",
              "   0.022360679774997918),\n",
              "  'd=100, window=5, min_count=3, iter=150, alpha=0.025': (0.84,\n",
              "   0.028284271247461888),\n",
              "  'd=100, window=5, min_count=4, iter=100, alpha=0.02': (0.9049999999999999,\n",
              "   0.04330127018922194),\n",
              "  'd=100, window=5, min_count=4, iter=100, alpha=0.025': (0.895,\n",
              "   0.021794494717703387),\n",
              "  'd=100, window=5, min_count=4, iter=150, alpha=0.02': (0.86,\n",
              "   0.014142135623730963),\n",
              "  'd=100, window=5, min_count=4, iter=150, alpha=0.025': (0.8350000000000001,\n",
              "   0.03570714214271423),\n",
              "  'd=100, window=6, min_count=2, iter=100, alpha=0.02': (0.9249999999999999,\n",
              "   0.016583123951776964),\n",
              "  'd=100, window=6, min_count=2, iter=100, alpha=0.025': (0.915,\n",
              "   0.01658312395177697),\n",
              "  'd=100, window=6, min_count=2, iter=150, alpha=0.02': (0.875,\n",
              "   0.021794494717703387),\n",
              "  'd=100, window=6, min_count=2, iter=150, alpha=0.025': (0.86,\n",
              "   0.014142135623730963),\n",
              "  'd=100, window=6, min_count=3, iter=100, alpha=0.02': (0.92,\n",
              "   0.024494897427831758),\n",
              "  'd=100, window=6, min_count=3, iter=100, alpha=0.025': (0.86,\n",
              "   0.0374165738677394),\n",
              "  'd=100, window=6, min_count=3, iter=150, alpha=0.02': (0.86,\n",
              "   0.028284271247461926),\n",
              "  'd=100, window=6, min_count=3, iter=150, alpha=0.025': (0.825,\n",
              "   0.021794494717703356),\n",
              "  'd=100, window=6, min_count=4, iter=100, alpha=0.02': (0.89,\n",
              "   0.01732050807568879),\n",
              "  'd=100, window=6, min_count=4, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "   0.029580398915498074),\n",
              "  'd=100, window=6, min_count=4, iter=150, alpha=0.02': (0.85,\n",
              "   0.02999999999999998),\n",
              "  'd=100, window=6, min_count=4, iter=150, alpha=0.025': (0.855,\n",
              "   0.008660254037844395),\n",
              "  'd=200, window=3, min_count=2, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "   0.03162277660168377),\n",
              "  'd=200, window=3, min_count=2, iter=100, alpha=0.025': (0.9349999999999999,\n",
              "   0.016583123951776964),\n",
              "  'd=200, window=3, min_count=2, iter=150, alpha=0.02': (0.9199999999999999,\n",
              "   0.024494897427831758),\n",
              "  'd=200, window=3, min_count=2, iter=150, alpha=0.025': (0.89,\n",
              "   0.03605551275463988),\n",
              "  'd=200, window=3, min_count=3, iter=100, alpha=0.02': (0.89,\n",
              "   0.010000000000000009),\n",
              "  'd=200, window=3, min_count=3, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "   0.055452682532047076),\n",
              "  'd=200, window=3, min_count=3, iter=150, alpha=0.02': (0.905,\n",
              "   0.025980762113533142),\n",
              "  'd=200, window=3, min_count=3, iter=150, alpha=0.025': (0.86,\n",
              "   0.014142135623730963),\n",
              "  'd=200, window=3, min_count=4, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "   0.024494897427831758),\n",
              "  'd=200, window=3, min_count=4, iter=100, alpha=0.025': (0.8999999999999999,\n",
              "   0.03741657386773942),\n",
              "  'd=200, window=3, min_count=4, iter=150, alpha=0.02': (0.905,\n",
              "   0.008660254037844395),\n",
              "  'd=200, window=3, min_count=4, iter=150, alpha=0.025': (0.865,\n",
              "   0.03570714214271428),\n",
              "  'd=200, window=4, min_count=2, iter=100, alpha=0.02': (0.945,\n",
              "   0.029580398915498064),\n",
              "  'd=200, window=4, min_count=2, iter=100, alpha=0.025': (0.915,\n",
              "   0.008660254037844395),\n",
              "  'd=200, window=4, min_count=2, iter=150, alpha=0.02': (0.9,\n",
              "   0.014142135623730963),\n",
              "  'd=200, window=4, min_count=2, iter=150, alpha=0.025': (0.865,\n",
              "   0.03570714214271428),\n",
              "  'd=200, window=4, min_count=3, iter=100, alpha=0.02': (0.895,\n",
              "   0.016583123951777013),\n",
              "  'd=200, window=4, min_count=3, iter=100, alpha=0.025': (0.91,\n",
              "   0.02236067977499788),\n",
              "  'd=200, window=4, min_count=3, iter=150, alpha=0.02': (0.905,\n",
              "   0.016583123951777013),\n",
              "  'd=200, window=4, min_count=3, iter=150, alpha=0.025': (0.875,\n",
              "   0.02598076211353318),\n",
              "  'd=200, window=4, min_count=4, iter=100, alpha=0.02': (0.94,\n",
              "   0.024494897427831758),\n",
              "  'd=200, window=4, min_count=4, iter=100, alpha=0.025': (0.9,\n",
              "   0.014142135623730963),\n",
              "  'd=200, window=4, min_count=4, iter=150, alpha=0.02': (0.9,\n",
              "   0.028284271247461888),\n",
              "  'd=200, window=4, min_count=4, iter=150, alpha=0.025': (0.845,\n",
              "   0.008660254037844395),\n",
              "  'd=200, window=5, min_count=2, iter=100, alpha=0.02': (0.9,\n",
              "   0.014142135623730963),\n",
              "  'd=200, window=5, min_count=2, iter=100, alpha=0.025': (0.9149999999999999,\n",
              "   0.01658312395177697),\n",
              "  'd=200, window=5, min_count=2, iter=150, alpha=0.02': (0.895,\n",
              "   0.029580398915498064),\n",
              "  'd=200, window=5, min_count=2, iter=150, alpha=0.025': (0.8999999999999999,\n",
              "   0.024494897427831758),\n",
              "  'd=200, window=5, min_count=3, iter=100, alpha=0.02': (0.9249999999999999,\n",
              "   0.008660254037844347),\n",
              "  'd=200, window=5, min_count=3, iter=100, alpha=0.025': (0.885,\n",
              "   0.029580398915498105),\n",
              "  'd=200, window=5, min_count=3, iter=150, alpha=0.02': (0.88,\n",
              "   0.014142135623730963),\n",
              "  'd=200, window=5, min_count=3, iter=150, alpha=0.025': (0.875,\n",
              "   0.016583123951777013),\n",
              "  'd=200, window=5, min_count=4, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "   0.02236067977499788),\n",
              "  'd=200, window=5, min_count=4, iter=100, alpha=0.025': (0.905,\n",
              "   0.016583123951777013),\n",
              "  'd=200, window=5, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "   0.025980762113533184),\n",
              "  'd=200, window=5, min_count=4, iter=150, alpha=0.025': (0.85,\n",
              "   0.01732050807568879),\n",
              "  'd=200, window=6, min_count=2, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "   0.03),\n",
              "  'd=200, window=6, min_count=2, iter=100, alpha=0.025': (0.8949999999999999,\n",
              "   0.029580398915498064),\n",
              "  'd=200, window=6, min_count=2, iter=150, alpha=0.02': (0.885,\n",
              "   0.016583123951777013),\n",
              "  'd=200, window=6, min_count=2, iter=150, alpha=0.025': (0.865,\n",
              "   0.03570714214271428),\n",
              "  'd=200, window=6, min_count=3, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "   0.01732050807568874),\n",
              "  'd=200, window=6, min_count=3, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "   0.03278719262150999),\n",
              "  'd=200, window=6, min_count=3, iter=150, alpha=0.02': (0.87,\n",
              "   0.010000000000000009),\n",
              "  'd=200, window=6, min_count=3, iter=150, alpha=0.025': (0.855,\n",
              "   0.016583123951777013),\n",
              "  'd=200, window=6, min_count=4, iter=100, alpha=0.02': (0.9,\n",
              "   0.014142135623730963),\n",
              "  'd=200, window=6, min_count=4, iter=100, alpha=0.025': (0.9249999999999999,\n",
              "   0.025980762113533135),\n",
              "  'd=200, window=6, min_count=4, iter=150, alpha=0.02': (0.89,\n",
              "   0.022360679774997918),\n",
              "  'd=200, window=6, min_count=4, iter=150, alpha=0.025': (0.85,\n",
              "   0.010000000000000009),\n",
              "  'd=300, window=3, min_count=2, iter=100, alpha=0.02': (0.945,\n",
              "   0.01658312395177697),\n",
              "  'd=300, window=3, min_count=2, iter=100, alpha=0.025': (0.915,\n",
              "   0.021794494717703356),\n",
              "  'd=300, window=3, min_count=2, iter=150, alpha=0.02': (0.905,\n",
              "   0.016583123951777013),\n",
              "  'd=300, window=3, min_count=2, iter=150, alpha=0.025': (0.905,\n",
              "   0.03840572873934303),\n",
              "  'd=300, window=3, min_count=3, iter=100, alpha=0.02': (0.9299999999999999,\n",
              "   0.02236067977499787),\n",
              "  'd=300, window=3, min_count=3, iter=100, alpha=0.025': (0.895,\n",
              "   0.008660254037844395),\n",
              "  'd=300, window=3, min_count=3, iter=150, alpha=0.02': (0.9099999999999999,\n",
              "   0.05385164807134505),\n",
              "  'd=300, window=3, min_count=3, iter=150, alpha=0.025': (0.895,\n",
              "   0.02598076211353318),\n",
              "  'd=300, window=3, min_count=4, iter=100, alpha=0.02': (0.945,\n",
              "   0.008660254037844395),\n",
              "  'd=300, window=3, min_count=4, iter=100, alpha=0.025': (0.92,\n",
              "   0.028284271247461888),\n",
              "  'd=300, window=3, min_count=4, iter=150, alpha=0.02': (0.89,\n",
              "   0.022360679774997918),\n",
              "  'd=300, window=3, min_count=4, iter=150, alpha=0.025': (0.89,\n",
              "   0.022360679774997918),\n",
              "  'd=300, window=4, min_count=2, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "   0.024494897427831758),\n",
              "  'd=300, window=4, min_count=2, iter=100, alpha=0.025': (0.9, 0.0),\n",
              "  'd=300, window=4, min_count=2, iter=150, alpha=0.02': (0.875,\n",
              "   0.016583123951777013),\n",
              "  'd=300, window=4, min_count=2, iter=150, alpha=0.025': (0.89,\n",
              "   0.04358898943540674),\n",
              "  'd=300, window=4, min_count=3, iter=100, alpha=0.02': (0.9299999999999999,\n",
              "   0.017320508075688742),\n",
              "  'd=300, window=4, min_count=3, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "   0.03570714214271423),\n",
              "  'd=300, window=4, min_count=3, iter=150, alpha=0.02': (0.875,\n",
              "   0.02598076211353318),\n",
              "  'd=300, window=4, min_count=3, iter=150, alpha=0.025': (0.855,\n",
              "   0.016583123951777013),\n",
              "  'd=300, window=4, min_count=4, iter=100, alpha=0.02': (0.915,\n",
              "   0.029580398915498064),\n",
              "  'd=300, window=4, min_count=4, iter=100, alpha=0.025': (0.9299999999999999,\n",
              "   0.009999999999999953),\n",
              "  'd=300, window=4, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "   0.04330127018922193),\n",
              "  'd=300, window=4, min_count=4, iter=150, alpha=0.025': (0.84,\n",
              "   0.028284271247461888),\n",
              "  'd=300, window=5, min_count=2, iter=100, alpha=0.02': (0.9349999999999999,\n",
              "   0.02598076211353313),\n",
              "  'd=300, window=5, min_count=2, iter=100, alpha=0.025': (0.9149999999999999,\n",
              "   0.021794494717703353),\n",
              "  'd=300, window=5, min_count=2, iter=150, alpha=0.02': (0.895,\n",
              "   0.021794494717703387),\n",
              "  'd=300, window=5, min_count=2, iter=150, alpha=0.025': (0.875,\n",
              "   0.03840572873934308),\n",
              "  'd=300, window=5, min_count=3, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "   0.03),\n",
              "  'd=300, window=5, min_count=3, iter=100, alpha=0.025': (0.9249999999999999,\n",
              "   0.016583123951776964),\n",
              "  'd=300, window=5, min_count=3, iter=150, alpha=0.02': (0.895,\n",
              "   0.03570714214271425),\n",
              "  'd=300, window=5, min_count=3, iter=150, alpha=0.025': (0.85,\n",
              "   0.010000000000000009),\n",
              "  'd=300, window=5, min_count=4, iter=100, alpha=0.02': (0.905,\n",
              "   0.021794494717703342),\n",
              "  'd=300, window=5, min_count=4, iter=100, alpha=0.025': (0.9199999999999999,\n",
              "   0.014142135623730925),\n",
              "  'd=300, window=5, min_count=4, iter=150, alpha=0.02': (0.895,\n",
              "   0.016583123951777013),\n",
              "  'd=300, window=5, min_count=4, iter=150, alpha=0.025': (0.875,\n",
              "   0.02598076211353318),\n",
              "  'd=300, window=6, min_count=2, iter=100, alpha=0.02': (0.875,\n",
              "   0.02958039891549811),\n",
              "  'd=300, window=6, min_count=2, iter=100, alpha=0.025': (0.91,\n",
              "   0.010000000000000009),\n",
              "  'd=300, window=6, min_count=2, iter=150, alpha=0.02': (0.92,\n",
              "   0.014142135623730925),\n",
              "  'd=300, window=6, min_count=2, iter=150, alpha=0.025': (0.875,\n",
              "   0.03570714214271428),\n",
              "  'd=300, window=6, min_count=3, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "   0.03162277660168377),\n",
              "  'd=300, window=6, min_count=3, iter=100, alpha=0.025': (0.885,\n",
              "   0.016583123951777013),\n",
              "  'd=300, window=6, min_count=3, iter=150, alpha=0.02': (0.8999999999999999,\n",
              "   0.024494897427831758),\n",
              "  'd=300, window=6, min_count=3, iter=150, alpha=0.025': (0.855,\n",
              "   0.021794494717703387),\n",
              "  'd=300, window=6, min_count=4, iter=100, alpha=0.02': (0.89,\n",
              "   0.01732050807568879),\n",
              "  'd=300, window=6, min_count=4, iter=100, alpha=0.025': (0.895,\n",
              "   0.021794494717703387),\n",
              "  'd=300, window=6, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "   0.03840572873934307),\n",
              "  'd=300, window=6, min_count=4, iter=150, alpha=0.025': (0.85,\n",
              "   0.02999999999999998)},\n",
              " 144)"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_value = max(results.values())\n",
        "result = [(params, value) for params, value in results.items() if value == max_value]\n",
        "print(f\"Maximum achieved {result[0][1]} by using parameters {result[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgdxg0tE6syU",
        "outputId": "a1231683-548b-4a8c-d0a9-8fc4465abde4"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum achieved (0.945, 0.029580398915498064) by using parameters d=200, window=4, min_count=2, iter=100, alpha=0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_results = dict(sorted(results.items(), key=lambda item: item[1][0]))\n",
        "sorted_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAT2D9m-CwPL",
        "outputId": "1c5cad88-9272-42ac-f8e9-60acbcac8fb7"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d=100, window=4, min_count=4, iter=150, alpha=0.025': (0.825,\n",
              "  0.029580398915498064),\n",
              " 'd=100, window=6, min_count=3, iter=150, alpha=0.025': (0.825,\n",
              "  0.021794494717703356),\n",
              " 'd=100, window=3, min_count=2, iter=150, alpha=0.025': (0.8300000000000001,\n",
              "  0.03),\n",
              " 'd=100, window=5, min_count=4, iter=150, alpha=0.025': (0.8350000000000001,\n",
              "  0.03570714214271423),\n",
              " 'd=100, window=5, min_count=3, iter=150, alpha=0.025': (0.84,\n",
              "  0.028284271247461888),\n",
              " 'd=300, window=4, min_count=4, iter=150, alpha=0.025': (0.84,\n",
              "  0.028284271247461888),\n",
              " 'd=200, window=4, min_count=4, iter=150, alpha=0.025': (0.845,\n",
              "  0.008660254037844395),\n",
              " 'd=100, window=6, min_count=4, iter=150, alpha=0.02': (0.85,\n",
              "  0.02999999999999998),\n",
              " 'd=200, window=5, min_count=4, iter=150, alpha=0.025': (0.85,\n",
              "  0.01732050807568879),\n",
              " 'd=200, window=6, min_count=4, iter=150, alpha=0.025': (0.85,\n",
              "  0.010000000000000009),\n",
              " 'd=300, window=5, min_count=3, iter=150, alpha=0.025': (0.85,\n",
              "  0.010000000000000009),\n",
              " 'd=300, window=6, min_count=4, iter=150, alpha=0.025': (0.85,\n",
              "  0.02999999999999998),\n",
              " 'd=100, window=3, min_count=4, iter=150, alpha=0.02': (0.855,\n",
              "  0.029580398915498105),\n",
              " 'd=100, window=6, min_count=4, iter=150, alpha=0.025': (0.855,\n",
              "  0.008660254037844395),\n",
              " 'd=200, window=6, min_count=3, iter=150, alpha=0.025': (0.855,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=4, min_count=3, iter=150, alpha=0.025': (0.855,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=6, min_count=3, iter=150, alpha=0.025': (0.855,\n",
              "  0.021794494717703387),\n",
              " 'd=100, window=3, min_count=3, iter=150, alpha=0.025': (0.86,\n",
              "  0.020000000000000018),\n",
              " 'd=100, window=5, min_count=2, iter=150, alpha=0.02': (0.86,\n",
              "  0.042426406871192854),\n",
              " 'd=100, window=5, min_count=4, iter=150, alpha=0.02': (0.86,\n",
              "  0.014142135623730963),\n",
              " 'd=100, window=6, min_count=2, iter=150, alpha=0.025': (0.86,\n",
              "  0.014142135623730963),\n",
              " 'd=100, window=6, min_count=3, iter=100, alpha=0.025': (0.86,\n",
              "  0.0374165738677394),\n",
              " 'd=100, window=6, min_count=3, iter=150, alpha=0.02': (0.86,\n",
              "  0.028284271247461926),\n",
              " 'd=200, window=3, min_count=3, iter=150, alpha=0.025': (0.86,\n",
              "  0.014142135623730963),\n",
              " 'd=100, window=3, min_count=4, iter=150, alpha=0.025': (0.865,\n",
              "  0.021794494717703387),\n",
              " 'd=200, window=3, min_count=4, iter=150, alpha=0.025': (0.865,\n",
              "  0.03570714214271428),\n",
              " 'd=200, window=4, min_count=2, iter=150, alpha=0.025': (0.865,\n",
              "  0.03570714214271428),\n",
              " 'd=200, window=6, min_count=2, iter=150, alpha=0.025': (0.865,\n",
              "  0.03570714214271428),\n",
              " 'd=200, window=6, min_count=3, iter=150, alpha=0.02': (0.87,\n",
              "  0.010000000000000009),\n",
              " 'd=100, window=4, min_count=3, iter=150, alpha=0.025': (0.875,\n",
              "  0.029580398915498105),\n",
              " 'd=100, window=6, min_count=2, iter=150, alpha=0.02': (0.875,\n",
              "  0.021794494717703387),\n",
              " 'd=200, window=4, min_count=3, iter=150, alpha=0.025': (0.875,\n",
              "  0.02598076211353318),\n",
              " 'd=200, window=5, min_count=3, iter=150, alpha=0.025': (0.875,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=4, min_count=2, iter=150, alpha=0.02': (0.875,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=4, min_count=3, iter=150, alpha=0.02': (0.875,\n",
              "  0.02598076211353318),\n",
              " 'd=300, window=5, min_count=2, iter=150, alpha=0.025': (0.875,\n",
              "  0.03840572873934308),\n",
              " 'd=300, window=5, min_count=4, iter=150, alpha=0.025': (0.875,\n",
              "  0.02598076211353318),\n",
              " 'd=300, window=6, min_count=2, iter=100, alpha=0.02': (0.875,\n",
              "  0.02958039891549811),\n",
              " 'd=300, window=6, min_count=2, iter=150, alpha=0.025': (0.875,\n",
              "  0.03570714214271428),\n",
              " 'd=100, window=3, min_count=3, iter=100, alpha=0.025': (0.88,\n",
              "  0.014142135623730963),\n",
              " 'd=100, window=4, min_count=2, iter=150, alpha=0.025': (0.88,\n",
              "  0.03741657386773945),\n",
              " 'd=100, window=5, min_count=2, iter=150, alpha=0.025': (0.88,\n",
              "  0.020000000000000018),\n",
              " 'd=200, window=5, min_count=3, iter=150, alpha=0.02': (0.88,\n",
              "  0.014142135623730963),\n",
              " 'd=100, window=4, min_count=3, iter=150, alpha=0.02': (0.885,\n",
              "  0.008660254037844395),\n",
              " 'd=100, window=4, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "  0.029580398915498105),\n",
              " 'd=200, window=5, min_count=3, iter=100, alpha=0.025': (0.885,\n",
              "  0.029580398915498105),\n",
              " 'd=200, window=5, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "  0.025980762113533184),\n",
              " 'd=200, window=6, min_count=2, iter=150, alpha=0.02': (0.885,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=4, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "  0.04330127018922193),\n",
              " 'd=300, window=6, min_count=3, iter=100, alpha=0.025': (0.885,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=6, min_count=4, iter=150, alpha=0.02': (0.885,\n",
              "  0.03840572873934307),\n",
              " 'd=100, window=3, min_count=3, iter=150, alpha=0.02': (0.8899999999999999,\n",
              "  0.02999999999999998),\n",
              " 'd=100, window=5, min_count=3, iter=150, alpha=0.02': (0.89,\n",
              "  0.022360679774997918),\n",
              " 'd=100, window=6, min_count=4, iter=100, alpha=0.02': (0.89,\n",
              "  0.01732050807568879),\n",
              " 'd=200, window=3, min_count=2, iter=150, alpha=0.025': (0.89,\n",
              "  0.03605551275463988),\n",
              " 'd=200, window=3, min_count=3, iter=100, alpha=0.02': (0.89,\n",
              "  0.010000000000000009),\n",
              " 'd=200, window=6, min_count=4, iter=150, alpha=0.02': (0.89,\n",
              "  0.022360679774997918),\n",
              " 'd=300, window=3, min_count=4, iter=150, alpha=0.02': (0.89,\n",
              "  0.022360679774997918),\n",
              " 'd=300, window=3, min_count=4, iter=150, alpha=0.025': (0.89,\n",
              "  0.022360679774997918),\n",
              " 'd=300, window=4, min_count=2, iter=150, alpha=0.025': (0.89,\n",
              "  0.04358898943540674),\n",
              " 'd=300, window=6, min_count=4, iter=100, alpha=0.02': (0.89,\n",
              "  0.01732050807568879),\n",
              " 'd=200, window=6, min_count=2, iter=100, alpha=0.025': (0.8949999999999999,\n",
              "  0.029580398915498064),\n",
              " 'd=100, window=3, min_count=4, iter=100, alpha=0.025': (0.895,\n",
              "  0.016583123951777013),\n",
              " 'd=100, window=5, min_count=2, iter=100, alpha=0.02': (0.895,\n",
              "  0.02598076211353318),\n",
              " 'd=100, window=5, min_count=3, iter=100, alpha=0.025': (0.895,\n",
              "  0.03570714214271425),\n",
              " 'd=100, window=5, min_count=4, iter=100, alpha=0.025': (0.895,\n",
              "  0.021794494717703387),\n",
              " 'd=200, window=4, min_count=3, iter=100, alpha=0.02': (0.895,\n",
              "  0.016583123951777013),\n",
              " 'd=200, window=5, min_count=2, iter=150, alpha=0.02': (0.895,\n",
              "  0.029580398915498064),\n",
              " 'd=300, window=3, min_count=3, iter=100, alpha=0.025': (0.895,\n",
              "  0.008660254037844395),\n",
              " 'd=300, window=3, min_count=3, iter=150, alpha=0.025': (0.895,\n",
              "  0.02598076211353318),\n",
              " 'd=300, window=5, min_count=2, iter=150, alpha=0.02': (0.895,\n",
              "  0.021794494717703387),\n",
              " 'd=300, window=5, min_count=3, iter=150, alpha=0.02': (0.895,\n",
              "  0.03570714214271425),\n",
              " 'd=300, window=5, min_count=4, iter=150, alpha=0.02': (0.895,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=6, min_count=4, iter=100, alpha=0.025': (0.895,\n",
              "  0.021794494717703387),\n",
              " 'd=100, window=3, min_count=4, iter=100, alpha=0.02': (0.8999999999999999,\n",
              "  0.028284271247461888),\n",
              " 'd=100, window=5, min_count=2, iter=100, alpha=0.025': (0.8999999999999999,\n",
              "  0.042426406871192854),\n",
              " 'd=200, window=3, min_count=4, iter=100, alpha=0.025': (0.8999999999999999,\n",
              "  0.03741657386773942),\n",
              " 'd=200, window=5, min_count=2, iter=150, alpha=0.025': (0.8999999999999999,\n",
              "  0.024494897427831758),\n",
              " 'd=300, window=6, min_count=3, iter=150, alpha=0.02': (0.8999999999999999,\n",
              "  0.024494897427831758),\n",
              " 'd=100, window=4, min_count=4, iter=100, alpha=0.025': (0.9,\n",
              "  0.042426406871192854),\n",
              " 'd=100, window=5, min_count=3, iter=100, alpha=0.02': (0.9,\n",
              "  0.024494897427831803),\n",
              " 'd=200, window=4, min_count=2, iter=150, alpha=0.02': (0.9,\n",
              "  0.014142135623730963),\n",
              " 'd=200, window=4, min_count=4, iter=100, alpha=0.025': (0.9,\n",
              "  0.014142135623730963),\n",
              " 'd=200, window=4, min_count=4, iter=150, alpha=0.02': (0.9,\n",
              "  0.028284271247461888),\n",
              " 'd=200, window=5, min_count=2, iter=100, alpha=0.02': (0.9,\n",
              "  0.014142135623730963),\n",
              " 'd=200, window=6, min_count=4, iter=100, alpha=0.02': (0.9,\n",
              "  0.014142135623730963),\n",
              " 'd=300, window=4, min_count=2, iter=100, alpha=0.025': (0.9, 0.0),\n",
              " 'd=100, window=4, min_count=4, iter=100, alpha=0.02': (0.9049999999999999,\n",
              "  0.049749371855331),\n",
              " 'd=100, window=5, min_count=4, iter=100, alpha=0.02': (0.9049999999999999,\n",
              "  0.04330127018922194),\n",
              " 'd=100, window=6, min_count=4, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "  0.029580398915498074),\n",
              " 'd=200, window=3, min_count=3, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "  0.055452682532047076),\n",
              " 'd=200, window=6, min_count=3, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "  0.03278719262150999),\n",
              " 'd=300, window=4, min_count=3, iter=100, alpha=0.025': (0.9049999999999999,\n",
              "  0.03570714214271423),\n",
              " 'd=100, window=3, min_count=2, iter=150, alpha=0.02': (0.905,\n",
              "  0.016583123951777013),\n",
              " 'd=100, window=4, min_count=2, iter=100, alpha=0.025': (0.905,\n",
              "  0.016583123951777013),\n",
              " 'd=100, window=4, min_count=2, iter=150, alpha=0.02': (0.905,\n",
              "  0.05172040216394301),\n",
              " 'd=100, window=4, min_count=3, iter=100, alpha=0.025': (0.905,\n",
              "  0.025980762113533142),\n",
              " 'd=200, window=3, min_count=3, iter=150, alpha=0.02': (0.905,\n",
              "  0.025980762113533142),\n",
              " 'd=200, window=3, min_count=4, iter=150, alpha=0.02': (0.905,\n",
              "  0.008660254037844395),\n",
              " 'd=200, window=4, min_count=3, iter=150, alpha=0.02': (0.905,\n",
              "  0.016583123951777013),\n",
              " 'd=200, window=5, min_count=4, iter=100, alpha=0.025': (0.905,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=3, min_count=2, iter=150, alpha=0.02': (0.905,\n",
              "  0.016583123951777013),\n",
              " 'd=300, window=3, min_count=2, iter=150, alpha=0.025': (0.905,\n",
              "  0.03840572873934303),\n",
              " 'd=300, window=5, min_count=4, iter=100, alpha=0.02': (0.905,\n",
              "  0.021794494717703342),\n",
              " 'd=200, window=5, min_count=4, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "  0.02236067977499788),\n",
              " 'd=200, window=6, min_count=2, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "  0.03),\n",
              " 'd=200, window=6, min_count=3, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "  0.01732050807568874),\n",
              " 'd=300, window=3, min_count=3, iter=150, alpha=0.02': (0.9099999999999999,\n",
              "  0.05385164807134505),\n",
              " 'd=300, window=5, min_count=3, iter=100, alpha=0.02': (0.9099999999999999,\n",
              "  0.03),\n",
              " 'd=100, window=3, min_count=2, iter=100, alpha=0.025': (0.91,\n",
              "  0.01732050807568879),\n",
              " 'd=100, window=4, min_count=3, iter=100, alpha=0.02': (0.91,\n",
              "  0.01732050807568879),\n",
              " 'd=200, window=4, min_count=3, iter=100, alpha=0.025': (0.91,\n",
              "  0.02236067977499788),\n",
              " 'd=300, window=6, min_count=2, iter=100, alpha=0.025': (0.91,\n",
              "  0.010000000000000009),\n",
              " 'd=100, window=4, min_count=2, iter=100, alpha=0.02': (0.9149999999999999,\n",
              "  0.01658312395177697),\n",
              " 'd=200, window=5, min_count=2, iter=100, alpha=0.025': (0.9149999999999999,\n",
              "  0.01658312395177697),\n",
              " 'd=300, window=5, min_count=2, iter=100, alpha=0.025': (0.9149999999999999,\n",
              "  0.021794494717703353),\n",
              " 'd=100, window=3, min_count=3, iter=100, alpha=0.02': (0.915,\n",
              "  0.01658312395177697),\n",
              " 'd=100, window=6, min_count=2, iter=100, alpha=0.025': (0.915,\n",
              "  0.01658312395177697),\n",
              " 'd=200, window=4, min_count=2, iter=100, alpha=0.025': (0.915,\n",
              "  0.008660254037844395),\n",
              " 'd=300, window=3, min_count=2, iter=100, alpha=0.025': (0.915,\n",
              "  0.021794494717703356),\n",
              " 'd=300, window=4, min_count=4, iter=100, alpha=0.02': (0.915,\n",
              "  0.029580398915498064),\n",
              " 'd=200, window=3, min_count=2, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "  0.03162277660168377),\n",
              " 'd=200, window=3, min_count=2, iter=150, alpha=0.02': (0.9199999999999999,\n",
              "  0.024494897427831758),\n",
              " 'd=200, window=3, min_count=4, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "  0.024494897427831758),\n",
              " 'd=300, window=4, min_count=2, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "  0.024494897427831758),\n",
              " 'd=300, window=5, min_count=4, iter=100, alpha=0.025': (0.9199999999999999,\n",
              "  0.014142135623730925),\n",
              " 'd=300, window=6, min_count=3, iter=100, alpha=0.02': (0.9199999999999999,\n",
              "  0.03162277660168377),\n",
              " 'd=100, window=6, min_count=3, iter=100, alpha=0.02': (0.92,\n",
              "  0.024494897427831758),\n",
              " 'd=300, window=3, min_count=4, iter=100, alpha=0.025': (0.92,\n",
              "  0.028284271247461888),\n",
              " 'd=300, window=6, min_count=2, iter=150, alpha=0.02': (0.92,\n",
              "  0.014142135623730925),\n",
              " 'd=100, window=3, min_count=2, iter=100, alpha=0.02': (0.9249999999999999,\n",
              "  0.02958039891549806),\n",
              " 'd=100, window=6, min_count=2, iter=100, alpha=0.02': (0.9249999999999999,\n",
              "  0.016583123951776964),\n",
              " 'd=200, window=5, min_count=3, iter=100, alpha=0.02': (0.9249999999999999,\n",
              "  0.008660254037844347),\n",
              " 'd=200, window=6, min_count=4, iter=100, alpha=0.025': (0.9249999999999999,\n",
              "  0.025980762113533135),\n",
              " 'd=300, window=5, min_count=3, iter=100, alpha=0.025': (0.9249999999999999,\n",
              "  0.016583123951776964),\n",
              " 'd=300, window=3, min_count=3, iter=100, alpha=0.02': (0.9299999999999999,\n",
              "  0.02236067977499787),\n",
              " 'd=300, window=4, min_count=3, iter=100, alpha=0.02': (0.9299999999999999,\n",
              "  0.017320508075688742),\n",
              " 'd=300, window=4, min_count=4, iter=100, alpha=0.025': (0.9299999999999999,\n",
              "  0.009999999999999953),\n",
              " 'd=200, window=3, min_count=2, iter=100, alpha=0.025': (0.9349999999999999,\n",
              "  0.016583123951776964),\n",
              " 'd=300, window=5, min_count=2, iter=100, alpha=0.02': (0.9349999999999999,\n",
              "  0.02598076211353313),\n",
              " 'd=200, window=4, min_count=4, iter=100, alpha=0.02': (0.94,\n",
              "  0.024494897427831758),\n",
              " 'd=200, window=4, min_count=2, iter=100, alpha=0.02': (0.945,\n",
              "  0.029580398915498064),\n",
              " 'd=300, window=3, min_count=2, iter=100, alpha=0.02': (0.945,\n",
              "  0.01658312395177697),\n",
              " 'd=300, window=3, min_count=4, iter=100, alpha=0.02': (0.945,\n",
              "  0.008660254037844395)}"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "frC9LQ47H4Iz"
      },
      "execution_count": 237,
      "outputs": []
    }
  ]
}